{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAi4viOPe3wXLo7mm4v74c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asmamest/tp_bigdata/blob/main/tp_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8Dawtg8iyAu",
        "outputId": "506357e8-716a-4128-97f1-5483761e81b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Création de la session Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TP_Spark\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# sc est le SparkContext (important pour les RDD)\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "kHQ4FPQfjIvO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installer Spark\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Configurer les variables d'environnement\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt4KzaKCjwr6",
        "outputId": "8a313bbe-f585-4b3d-b09a-fb7270030328"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (108.157.173.97)] [C\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [2 InRelease 15.6 kB/128 kB 12%] [3 InRelease 28.7 kB/129 kB 22%] [Waiting f\r                                                                               \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [2 InRelease 15.6 kB/128 kB 12%] [3 InRelease 43.1 kB/129 kB 33%] [Connected\r                                                                               \rGet:5 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\r0% [2 InRelease 15.6 kB/128 kB 12%] [3 InRelease 43.1 kB/129 kB 33%] [Connected\r0% [2 InRelease 15.6 kB/128 kB 12%] [3 InRelease 46.0 kB/129 kB 36%] [Connected\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [2 InRelease 59.1 kB/128 kB 46%] [3 InRelease 110 kB/129 kB 85%] [Connected \r0% [2 InRelease 70.6 kB/128 kB 55%] [Waiting for headers] [Connected to ppa.lau\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rGet:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://cli.github.com/packages stable/main amd64 Packages [344 B]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,279 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,429 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,988 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,778 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,586 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,811 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,361 kB]\n",
            "Fetched 28.7 MB in 3s (9,737 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Télécharger le fichier Shakespeare\n",
        "!wget -q https://raw.githubusercontent.com/apache/spark/master/data/mllib/shakespeare.txt\n",
        "\n",
        "# Vérifier que le fichier est bien téléchargé\n",
        "!ls -la shakespeare.txt"
      ],
      "metadata": {
        "id": "IaCrLJ_oltAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Création de la session Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TP_Spark_Colab\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Step 3 : Lecture du fichier (CORRECTION - utiliser le bon nom de fichier)\n",
        "shakespeare_rdd = sc.textFile(\"shakespeare.txt\")\n",
        "\n",
        "# Vérification du chargement\n",
        "print(\"Nombre de lignes:\", shakespeare_rdd.count())\n",
        "print(\"Type de l'opération textFile:\", type(shakespeare_rdd))\n",
        "print(\"Premières lignes:\")\n",
        "shakespeare_rdd.take(5)"
      ],
      "metadata": {
        "id": "Z4bFuHdklv0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4 : Division en mots\n",
        "words_rdd = shakespeare_rdd.flatMap(lambda line: line.split())\n",
        "print(\"Nombre total de mots:\", words_rdd.count())\n",
        "\n",
        "# Step 5 : Transformation en paires (clé, valeur)\n",
        "word_pairs_rdd = words_rdd.map(lambda word: (word.lower().strip('.,!?;:\"()'), 1))\n",
        "print(\"Exemples de paires mot-valeur:\")\n",
        "word_pairs_rdd.take(10)\n",
        "\n",
        "# Step 6 : Réduction par clé\n",
        "word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
        "print(\"10 mots les plus fréquents:\")\n",
        "sorted_counts = word_counts_rdd.sortBy(lambda x: x[1], ascending=False)\n",
        "sorted_counts.take(10)\n",
        "\n",
        "# Step 7 : Sauvegarde du résultat\n",
        "!rm -rf word_counts_output  # Nettoyer si existe déjà\n",
        "\n",
        "word_counts_rdd.coalesce(1).saveAsTextFile(\"word_counts_output\")\n",
        "\n",
        "# Vérification du résultat\n",
        "!ls word_counts_output/\n",
        "result_rdd = sc.textFile(\"word_counts_output/part-00000\")\n",
        "print(\"Résultat sauvegardé:\")\n",
        "result_rdd.take(20)"
      ],
      "metadata": {
        "id": "eSrjvXStmEtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Télécharger le dataset daily_weather\n",
        "!wget -q https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily_weather.csv\n",
        "\n",
        "# Vérification\n",
        "!ls -la daily_weather.csv"
      ],
      "metadata": {
        "id": "ebpfNpKjmIjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 : Chargement des données\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Chargement du fichier CSV\n",
        "df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load(\"daily_weather.csv\")\n",
        "\n",
        "print(\"Colonnes:\", df.columns)\n",
        "print(\"Nombre de lignes:\", df.count())\n",
        "\n",
        "# Step 2 : Visualisation des données\n",
        "print(\"\\nSchéma:\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\n5 premiers éléments:\")\n",
        "df.show(5)\n",
        "\n",
        "# Step 3 : Statistiques récapitulatives\n",
        "print(\"Statistiques récapitulatives:\")\n",
        "df.describe().show()\n",
        "\n",
        "# Step 4 : Manipulations avancées\n",
        "# 1. Création de la colonne ratio (avec gestion des divisions par zéro)\n",
        "df1 = df.withColumn(\"ratio\",\n",
        "                   when(col(\"rain_accumulation_9am\") != 0,\n",
        "                        col(\"rain_duration_9am\") / col(\"rain_accumulation_9am\"))\n",
        "                   .otherwise(None))\n",
        "\n",
        "# 2. Affichage de la nouvelle colonne\n",
        "print(\"Colonne ratio:\")\n",
        "df1.select(\"rain_duration_9am\", \"rain_accumulation_9am\", \"ratio\").show(10)\n",
        "\n",
        "# 3. Maximum de rain_duration_9am\n",
        "from pyspark.sql.functions import max, min, mean, count\n",
        "\n",
        "max_duration = df.agg(max(\"rain_duration_9am\")).first()[0]\n",
        "print(f\"Maximum rain_duration_9am: {max_duration}\")\n",
        "\n",
        "# 4. Moyenne de rain_accumulation_9am\n",
        "mean_accumulation = df.agg(mean(\"rain_accumulation_9am\")).first()[0]\n",
        "print(f\"Moyenne rain_accumulation_9am: {mean_accumulation}\")\n",
        "\n",
        "# 5. Max et min de rain_duration_9am\n",
        "max_min = df.agg(max(\"rain_duration_9am\"), min(\"rain_duration_9am\")).first()\n",
        "print(f\"Max rain_duration_9am: {max_min[0]}, Min: {max_min[1]}\")\n",
        "\n",
        "# 6. Compter les lignes où air_temp_9am > 70\n",
        "count_high_temp = df.filter(col(\"air_temp_9am\") > 70).count()\n",
        "print(f\"Nombre de fois où air_temp_9am > 70: {count_high_temp}\")"
      ],
      "metadata": {
        "id": "iAaVQVrDmLPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5 : Suppression des valeurs manquantes\n",
        "print(f\"Lignes avant nettoyage: {df.count()}\")\n",
        "\n",
        "# Supprimer les lignes avec des valeurs manquantes dans air_pressure_9am\n",
        "df_clean = df.filter(col(\"air_pressure_9am\").isNotNull())\n",
        "print(f\"Lignes après nettoyage air_pressure_9am: {df_clean.count()}\")\n",
        "\n",
        "# Step 6 : Calcul de corrélation\n",
        "correlation = df.corr(\"rain_accumulation_9am\", \"rain_duration_9am\")\n",
        "print(f\"Corrélation entre rain_accumulation_9am et rain_duration_9am: {correlation:.4f}\")\n",
        "\n",
        "# Interprétation :\n",
        "# Une corrélation proche de 1 ou -1 indique une forte relation linéaire\n",
        "# Une corrélation proche de 0 indique une faible relation linéaire"
      ],
      "metadata": {
        "id": "0b9250fsmOIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Faire une copie du DataFrame\n",
        "df_imputed = df.alias(\"df_imputed\")\n",
        "\n",
        "# Liste des colonnes numériques pour l'imputation\n",
        "numeric_columns = [col for col, dtype in df.dtypes if dtype in ['int', 'double', 'float']]\n",
        "\n",
        "print(\"Colonnes numériques pour imputation:\", numeric_columns)\n",
        "\n",
        "# Imputer les valeurs manquantes avec la moyenne\n",
        "for column in numeric_columns:\n",
        "    # Calculer la moyenne\n",
        "    mean_value = df.agg(avg(col(column))).first()[0]\n",
        "\n",
        "    # Imputer les valeurs manquantes\n",
        "    df_imputed = df_imputed.fillna(mean_value, subset=[column])\n",
        "    print(f\"Colonne {column}: valeur d'imputation = {mean_value:.2f}\")\n",
        "\n",
        "# Comparaison des statistiques\n",
        "print(\"=== AVANT IMPUTATION ===\")\n",
        "df.describe().show()\n",
        "\n",
        "print(\"=== APRÈS IMPUTATION ===\")\n",
        "df_imputed.describe().show()\n",
        "\n",
        "# Observation :\n",
        "# Les counts devraient être égaux après imputation\n",
        "# Les moyennes peuvent légèrement changer"
      ],
      "metadata": {
        "id": "N3tc5FHgmRUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arrêter la session Spark\n",
        "spark.stop()\n",
        "\n",
        "# Vérifier les fichiers créés\n",
        "print(\"Fichiers créés:\")\n",
        "!ls -la"
      ],
      "metadata": {
        "id": "-111MRVUmWIo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}